<?xml version="1.0" encoding="UTF-8"?>
<quiz xmlns="https://bigoquiz.com/document" format_version="1" id="distributed_systems">
    <title>Distributed Systems</title>

    <section id="theory" answers_as_choices="true" and_reverse="true">
        <title>Theory</title>
        <!-- Much of this is inspired by http://bravenewgeek.com/from-the-ground-up-reasoning-about-distributed-systems-in-the-real-world/ -->

        <question id="theory-cap">
            <text>CAP Theorem (Brewer)</text>
            <link>https://en.wikipedia.org/wiki/CAP_theorem</link>
            <answer>It is impossible for a distributed computer system to provide more than 2 of these guarantees: Consistency, Availability, and Partition Tolerance.</answer>
        </question>

        <question id="theory-flp-result">
            <text>FLP Result (Fischer, Lynch, and Patterson)</text>
            <link>https://en.wikipedia.org/wiki/CAP_theorem</link>
            <answer>Impossibility of distributed consensus with one faulty process.</answer>
        </question>

        <question id="theory-two-generals-problem">
            <text>Two Generals Problem</text>
            <link>https://en.wikipedia.org/wiki/Two_Generals'_Problem</link>
            <answer>Coordinating an action between two actors via an unreliable link. Proved unsolvable.</answer>
            <note>Mitigated by accepting the unreliability of the link, for instance by sending duplicate messages.</note>
        </question>

        <question id="theory-byzantine-generals-problem">
            <text>Byzantine Generals Problem (Lamport)</text>
            <link>https://en.wikipedia.org/wiki/Byzantine_fault_tolerance#Byzantine_Generals.27_Problem</link>
            <answer>Coordinating an action via an unreliable link when the other actors may be unreliable or even traitorous.</answer>
            <note>Byzantine fault tolerance is possible if the loyal (non-faulty) generals agree unanimously.</note>
            <!-- When is this unsolvable? Wikipedia says "unsolvable in the face of arbitrary communication failures" though only on the 2 generals problem page: https://en.wikipedia.org/wiki/Two_Generals%27_Problem -->
        </question>

        <!-- This is more of a generic concurrency problem (deadlock) than a distributed-computing problem:
        <question id="theory-dining-philosophers-problem">
            <text>Dining Philosopher's Problem (Dijkstra)</text>
            <link>https://en.wikipedia.org/wiki/Dining_philosophers_problem</link>
            <answer>TODO</answer>
        </question>
        -->
    </section>

    <section id="golden-signals" answers_as_choices="true" and_reverse="true">
        <title>Google SRE: Four Golden Signals</title>

        <question id="golden-signals-latency">
            <text>Latency</text>
            <link>https://en.wikipedia.org/wiki/Latency_(engineering)</link>
            <answer>Time needed to service a request.</answer>
            <note>Different requests to the same service may have different latencies.</note>
        </question>

        <question id="golden-signals-traffic">
            <text>Traffic (throughput)</text>
            <link>https://en.wikipedia.org/wiki/Throughput</link>
            <answer>Rate of requests.</answer>
        </question>

        <question id="golden-signals-errors">
            <text>Errors</text>
            <answer>Rate of failing requests.</answer>
        </question>

        <question id="golden-signals-saturation">
            <text>Saturation</text>
            <answer>How fully a system's resources are used.</answer>
        </question>
    </section>

    <section id="hierarchy-of-reliability" answers_as_choices="true">
        <title>Mikey Dickerson's Hierarchy of Reliability</title>
        <link>https://docs.google.com/drawings/d/1kshrK2RLkW-XV8enmWZxeRFRgADj6d4Ru_w5txz_k9I/edit</link>
        <!-- TODO: Find a better link -->

        <question id="hierarchy-of-reliability-order">
            <text>Most fundamental first</text>
            <answer>Monitoring, Incident Response, Postmortem/Root Cause Analysis, Testing &amp; Release Procedures, Capacity Planning, Development, Product</answer>
        </question>

        <question id="hierarchy-of-reliability-reverse-order">
            <text>Most dependent first</text>
            <answer>Product, Capacity Planning, Development, Testing &amp; Release Procedures, Postmortem/Root Cause Analysis, Incident Response, Monitoring</answer>
        </question>
    </section>

    <section id="other-terminology" answers_as_choices="true" and_reverse="true">
        <title>Other Terminology</title>

        <question id="terminology-slo">
            <text>SLO</text>
            <answer>Service Level Objective</answer>
            <!-- TODO: How is it measured, for instance? -->
        </question>

        <question id="terminology-sla">
            <text>SLA</text>
            <answer>Service Level Agreement</answer>
        </question>

        <question id="terminology-failure-modes">
            <text>Failure Modes</text>
            <answer>The various things that can go wrong and how the overall service will behave.</answer>
            <note>For instance, when a machine dies, when a rack fails, when a cluster fails, when a datacenter fails, when a network fails between datacenters.</note>
        </question>
        <!-- TODO: Single point of failure. Unavailable dependencies. Request Deadlines. Deadline Propagation -->

        <question id="terminology-load-balancing">
            <text>Load Balancing</text>
            <link>https://en.wikipedia.org/wiki/Load_balancing_(computing)</link>
            <answer>Distributes incoming network traffic across several backend servers.</answer>
            <note>For instance, using DNS, or at the virtual IP address.</note>
        </question>

        <question id="terminology-load-shedding">
            <text>Load Shedding</text>
            <link>https://en.wikipedia.org/wiki/Load_Shedding</link>
            <answer>Ignore some requests to reduce load on a server so it may serve at least some requests.</answer>
        </question>

        <question id="terminology-graceful-degradation">
            <text>Graceful Degradation</text>
            <link>https://en.wikipedia.org/w/index.php?title=Graceful_degradation</link>
            <answer>Reduce the amount of work to be performed for each request.</answer>
        </question>

        <question id="terminology-rate-limiting">
            <text>Rate Limiting</text>
            <link>https://en.wikipedia.org/wiki/Rate_limiting</link>
            <answer>Controls the rate of traffic.</answer>
            <note>Can happen at reverse proxies or at the load balancers.</note>
        </question>

        <question id="terminology-sharding">
            <text>Sharding</text>
            <answer>Splitting databases across servers.</answer>
        </question>

        <question id="terminology-rollback">
            <text>Rollback</text>
            <link>https://en.wikipedia.org/wiki/Rollback_(data_management)</link>
            <answer>Returning data or software to previous states or versions.</answer>
        </question>

        <question id="terminology-exponential-backoff">
            <text>Exponential Backoff</text>
            <link>https://en.wikipedia.org/wiki/Exponential_backoff</link>
            <answer>Clients delay retries by increasing (by multiplication) intervals.</answer>
            <note>See also Jitter</note>
        </question>

        <question id="terminology-jitter">
            <text>Jitter</text>
            <link>https://www.awsarchitectureblog.com/2015/03/backoff.html</link>
            <answer>Adds randomness to retry delays to avoid clustering of requests.</answer>
            <note>See also Exponential Backoff</note>
        </question>

        <question id="terminology-vertical-scaling">
            <text>Vertical Scaling</text>
            <link>https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling</link>
            <answer>Add resources (CPU, RAM, disk) to existing servers.</answer>
        </question>

        <question id="terminology-horizontal-scaling">
            <text>Horizontal Scaling</text>
            <link>https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling</link>
            <answer>Add servers.</answer>
        </question>

        <question id="terminology-cascading-failure">
            <text>Cascading Failure</text>
            <link>https://en.wikipedia.org/wiki/Cascading_failure</link>
            <answer>When a failure triggers failures of successive parts of a system.</answer>
        </question>

        <question id="terminology-consistent-hashing">
            <text>Consistent Hashing</text>
            <link>https://en.wikipedia.org/wiki/Consistent_hashing</link>
            <answer>Allows sharding across servers, without needing rehashing and resharding when adding or removing a server.</answer>
            <video_url>https://youtu.be/PQY2_QopWTM</video_url>
            <!-- Also good: https://www.youtube.com/watch?v=jznJKL0CrxM -->
            <note>Keys and their values are stored in whatever node's hashed ID is a successor to the hash of the key. The nodes are considered to be in a ring, so there is always a successor. Keys are stored in multiple nodes, either by using successor nodes or by using multiple hash functions, so we don't lose data when a node fails. Each node can identify the node responsible for a key, for instance with a finger table.</note>
        </question>

        <!-- TODO?
        <question id="terminology-finger-tables">
            <text>Finger Tables</text>
            <link>https://en.wikipedia.org/wiki/Chord_(peer-to-peer)#Finger_table</link>
            <answer>TODO: O(log n) hops.</answer>
            <video_url>https://www.youtube.com/watch?v=GOOXa2GkPws</video_url>
        </question>
        -->

        <question id="terminology-eventual-consistency">
            <text>Eventual Consistency</text>
            <link>https://en.wikipedia.org/wiki/Eventual_consistency</link>
            <answer>Offers high availability by only ensuring that reads will all eventually return the latest value (converge), instead of guaranteeing consistency at all times.</answer>
            <note>This requires more conflict resolution.</note>
        </question>

        <question id="terminology-read-replication">
            <text>Read Replication</text>
            <answer>Writes are propagated to multiple replicas which serve only reads.</answer>
            <video_url>https://youtu.be/MbqAH5OzLjE</video_url>
        </question>

        <question id="terminology-consistent-scatter-gather">
            <text>Scatter/Gather</text>
            <answer>Request data from multiple servers, aggregate them on one server, and return an overall response.</answer>
        </question>

        <question id="terminology-map-reduce">
            <text>MapReduce</text>
            <link>https://en.wikipedia.org/wiki/MapReduce</link>
            <answer>Distributed batch processing of key/value pairs.</answer>
            <note>A Mapper splits key/value pairs into multiple key/value pairs, possibly outputting keys based on parts of the input value. The framework shuffles these keys, sorting and grouping equal keys together, and the Reducer then performs some aggregate calculation on these groups of key/value pairs.</note>
            <video_url>https://youtu.be/9CrWSHzy3BQ</video_url>
        </question>

        <question id="terminology-vector-clock">
            <text>Vector Clock</text>
            <link>https://en.wikipedia.org/wiki/Vector_clock</link>
            <answer>Discovers partial ordering of events and detects conflicts.</answer>
            <note>The vector of service IDs and associated sequence numbers is sent with every message. For each received message or sent message, the service increments its own sequence number, and uses the latest sequence number for other services. There is only a conflict when one message's vector clock is not contained within another message's vector clock. The application must resolve conflicts.</note>
            <video_url>https://youtu.be/IT5AqUEGLwM</video_url>
        </question>

        <question id="terminology-lamport-timestamp">
            <text>Lamport Timestamp</text>
            <link>https://en.wikipedia.org/wiki/Lamport_timestamps</link>
            <answer>Discovers partial ordering of events and detect conflicts (simple ordering).</answer>
            <note>The timestamp is sent with every message. For each sent message, the service adds 1 to its timestamp. For every received message, it adds 1 to the max of its own timestamp and the received timestamp. However, this can suggest conflicts between causally-unrelated events - vector clocks let us detect actual conflicts.</note>
            <video_url>https://youtu.be/CMBjvCzDVkY</video_url>
            <!-- Also https://www.youtube.com/watch?v=jD4ECsieFbE -->
        </question>

        <question id="terminology-crdt">
            <text>CRDT</text>
            <link>https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type</link>
            <answer>Conflict-free Replicated Data Type (or Convergent Replicated Data Type)</answer>
        </question>

        <question id="terminology-liveness">
            <text>Liveness</text>
            <link>https://en.wikipedia.org/wiki/Liveness</link>
            <answer>A promise that the system will continue to make progress toward a correct result during concurrent operations. (It will terminate successfully.)</answer>
            <note>For instance, Eventual Consistency. For instance, having no deadlocks or resource starvation.</note>
        </question>

        <question id="terminology-safety">
            <text>Safety</text>
            <link>https://en.wikipedia.org/wiki/Safety_(distributed_computing)</link>
            <answer>A promise that something unwanted will never happen. (Partial correctness.)</answer>
            <note>For instance, a promise not to return null data.</note>
        </question>

        <question id="definition-circuit-breaker">
            <text>Circuit Breaker</text>
            <link>https://en.wikipedia.org/wiki/Circuit_breaker_design_pattern</link>
            <answer>Interrupts calls that take too long, and prevents calls to failing servers.</answer>
            <note>For instance, with Netflix's Hystrix.</note>
        </question>

        <question id="definition-bulkhead">
            <text>Bulkhead</text>
            <answer>Isolates sets of calls to particular services, to stop slow calls affecting calls to healthy services.</answer>
            <note>For instance, with Netflix's Hystrix, with divides sets of calls into different thread pools.</note>
        </question>

        <question id="definition-fallback">
            <text>Fallback</text>
            <answer>If the call to a service fails, use an alternative result, possibly from another service.</answer>
            <link>https://en.wikipedia.org/wiki/Circuit_breaker_design_pattern</link>
            <note>For instance, with Netflix's Hystrix.</note>
        </question>
    </section>

    <!-- These are based on chapters 19 and 20 of the Google SRE book:
         https://landing.google.com/sre/book/chapters/load-balancing-frontend.html
         https://landing.google.com/sre/book/chapters/load-balancing-datacenter.html
    The wikipedia page has some more. -->
    <section id="load-balancing" answers_as_choices="true" and_reverse="true">
        <title>Load Balancing</title>
        <link>https://en.wikipedia.org/wiki/Load_balancing_(computing)</link>

        <question id="load_balancing-round-robin-dns">
            <text>Round Robin DNS</text>
            <link>https://en.wikipedia.org/wiki/Round-robin_DNS</link>
            <answer>The DNS servers respond with different IP Addresses each time.</answer>
            <note>This is of limited use because DNS records are cached at intermediate DNS servers and on clients.</note>
        </question>

        <question id="load_balancing-dns-delegation">
            <!-- TODO: This is my name for this. Is there an established name? -->
            <text>Geographically-optimal DNS</text>
            <!-- TODO: Better link? -->
            <link>https://landing.google.com/sre/book/chapters/load-balancing-frontend.html</link>
            <answer>The DNS servers respond with the IP address of a datacenter that is optimal for the user's location, based on the datacenter's location and capacity. Requires EDNS0.</answer>
            <note>This is of limited use because not all intermediate DNS servers support EDNS0, and they might not update their cached records when required.</note>
        </question>

        <question id="load_balancing-virtual-ip-link-layer">
            <text>Network Load Balancer: OSI Data Link layer / TCP/IP Link Layer</text>
            <link>https://en.wikipedia.org/wiki/Virtual_IP_address</link>
            <!-- TODO: Chapter 19 of the Google SRE book talks about changing the MAC address of packets, but it's really frames that
            are at the data link layer. -->
            <answer>The DNS server responds with the "virtual" IP address of a network load balancer that forwards the traffic to several backend servers, by changing the destination MAC address of the frames.</answer>
            <note>This allows DSR (Direct Server Response): The backend server responds directly to the client, without going through the load balancer. However, this requires all backend servers, and the load balancer, to be reachable at the data link layer.</note>
        </question>

        <question id="load_balancing-virtual-ip-packet-encapsulation">
            <text>Network Load Balancer: Packet Encapsulation</text>
            <link>https://en.wikipedia.org/wiki/Virtual_IP_address</link>
            <answer>The DNS server responds with the "virtual" IP address of a network load balancer that forwards the traffic to several backend servers, by encapsulating the IP packets in an outer packet with a backend server's destination IP address.</answer>
            <note>However, this increases the packet size, which can lead to fragmentation if the MTU (Maximum Transmission Unit) is exceeded.</note>
        </question>

        <question id="load-balancing-flow-control">
            <text>Flow Control</text>
            <answer>The client tracks the number of its own active requests on each backend server.</answer>
        </question>

        <question id="load-balancing-lame-duck-state">
            <text>Lame Duck State</text>
            <answer>Backend servers can ask clients to stop sending requests.</answer>
            <note>This also allows backend servers to shut down more cleanly, waiting until its has served its current queries.</note>
        </question>

        <question id="load-balancing-subsetting">
            <text>Subsetting</text>
            <answer>Limits the set of backend servers used by each client.</answer>
            <note>The client can then keep connections open even when no requests are active. Google drops inactive TCP/IP connections to UDP. Google uses "Deterministic Subsetting" to spread the load more evenly than with "Random Subsetting".</note>
        </question>

        <question id="load-balancing-round-robin">
            <text>Round Robin</text>
            <answer>The client sends requests to the next backend server in its subset that is not in a lame-duck state.</answer>
            <note>This does not spread the load evenly, because not all requests have the same cost, not all backend servers have the same capacity, and not all clients send the same number, or type, of requests.</note>
        </question>

        <question id="load-balancing-least-loaded-round-robin">
            <text>Least-Loaded Round Robin</text>
            <answer>The client tracks the number of its own active requests on each backend server, and sends requests to the next backend server in its subset that has a minimal number of active requests.</answer>
            <note>This spreads the load more evenly than simple round robin, but doesn't distinguish healthy servers from servers that are efficiently serving only errors. And it doesn't take into account backend servers that are waiting for other servers, and which could accept other requests in the meantime.</note>
        </question>

        <question id="load-balancing-weighted-round-robin">
            <text>Weighted Round Robin</text>
            <answer>The clients tracks the capability of the backend servers in its subset, based on performance metrics returned by each backend server along with each response or health check. The client sends requests to the next backend server in its subset according to the distribution of backend server capability.</answer>
        </question>
    </section>

    <!-- The answer text here is directly from the Wikipedia article. -->
    <section id="cap" answers_as_choices="true">
        <title>CAP Theorem</title>
        <link>https://en.wikipedia.org/wiki/CAP_theorem</link>

        <question id="cap-consistency">
            <text>Consistency</text>
            <answer>Every read receives the most recent write or an error.</answer>
            <note>Network partitions are inevitable, so consistency is possible (CP), but not both complete consistency and complete availability (CA).</note>
        </question>

        <question id="cap-availability">
            <text>Availability</text>
            <answer>Every request receives a response, without guarantee that it contains the most recent version of the information.</answer>
            <note>Network partitions are inevitable, so availability is possible (AP), but not both complete consistency and complete availability (CA).</note>
        </question>

        <question id="cap-partition-tolerance">
            <text>Partition Tolerance</text>
            <answer>The system continues to operate despite an arbitrary number of messages being dropped by the network between nodes.</answer>
            <note>Network partitions are inevitable, so any system must tend towards either CP or AP.</note>
        </question>
    </section>

    <section id="cap-protocols" answers_as_choices="true">
        <title>CAP Theorem: Example Protocols</title>
        <link>https://en.wikipedia.org/wiki/CAP_theorem</link>

        <question id="cap-protocols-ca">
            <text>CA (Consistency + Availability)</text>
            <answer>2PC (Two Phase Commit)</answer>
            <note>Assumes (unrealistically) that no network partitions will happen.</note>
        </question>

        <question id="cap-protocols-cp">
            <text>CP (Consistency + Partition Tolerance)</text>
            <answer>Paxos</answer>
            <note>Tolerates network partitions, by preventing writes to minority sets of nodes.</note>
        </question>

        <question id="cap-protocols-ap">
            <text>AP (Availability + Partition Tolerance)</text>
            <answer>Gossip</answer>
            <note>Tolerates network partitions (and node failures), by preventing any writes during failures.</note>
        </question>
    </section>

    <section id="consistency">
        <title>Consistency Models</title>
        <link>https://en.wikipedia.org/wiki/Consistency_model</link>

        <subsection id="consistency-strong-or-weak" answers_as_choices="true">
            <title>Strong or Weak</title>
            <link>http://book.mixu.net/distsys/abstractions.html#strong-consistency-vs-other-consistency-models</link>

            <question id="consistency-strong-or-weak-linearizable-consistency">
                <text>Linearizable Consistency</text>
                <answer>Strong</answer>
            </question>

            <question id="consistency-strong-or-weak-sequential-consistency">
                <text>Sequential Consistency</text>
                <answer>Strong</answer>
            </question>

            <question id="consistency-strong-or-weak-client-centric-consistency">
                <text>Client-centric Consistency</text>
                <answer>Weak</answer>
            </question>

            <question id="consistency-strong-or-weak-causal-consistency">
                <text>Causal Consistency</text>
                <answer>Weak</answer>
            </question>

            <question id="consistency-strong-or-weak-eventual-consistency">
                <text>Eventual Consistency</text>
                <answer>Weak</answer>
            </question>
        </subsection>

        <subsection id="consistency-strong-models" answers_as_choices="true" and_reverse="true">
            <title>Strong Consistency Models</title>
            <link>http://book.mixu.net/distsys/abstractions.html#strong-consistency-models</link>
            <!-- Also https://en.wikipedia.org/wiki/Consistency_model and
                 http://www.bailis.org/blog/linearizability-versus-serializability/ -->

            <question id="consistency-strong-models-linearizable">
                <text>Linearizable Consistency (Atomic Consistency)</text>
                <link>https://en.wikipedia.org/wiki/Linearizability</link>
                <answer>All operations appear to have executed atomically in an order that is consistent with the global real-time ordering. After a write, all later reads return that value or the value from a later write. After a read, all later reads return that value or the value from a later write.</answer>
                <note>Linearizability is sequential consistency with the real-time constraint.</note>
            </question>

            <question id="consistency-strong-models-sequential">
                <text>Sequential Consistency (Serializability)</text>
                <link>https://en.wikipedia.org/wiki/Sequential_consistency</link>
                <answer>All operations appear to have executed atomically in some order that is consistent with the order seen at individual nodes. Writes may not be seen immediately but written values will always be read in the same order by all nodes.</answer>
                <!-- TODO: Is Sequential Consistency really the same as serializability? See https://github.com/mixu/distsysbook/issues/34 -->
            </question>
        </subsection>

        <subsection id="consistency-weak-models" answers_as_choices="true" and_reverse="true">
            <title>Weak Consistency Models</title>

            <question id="consistency-weak-models-client-centric">
                <text>Client-centric Consistency</text>
                <answer>For instance, a client caches a result, and uses it instead if a node returns an older value.</answer>
            </question>

            <question id="consistency-weak-eventual">
                <text>Eventual Consistency</text>
                <answer>Reads will all eventually return the latest value (converge), but consistency is not guaranteed at all times.</answer>
                <link>https://en.wikipedia.org/wiki/Eventual_consistency</link>
            </question>

            <!-- TODO: Causal consistency? -->
        </subsection>
    </section>

    <section id="acid" answers_as_choices="true">
        <title>ACID</title>
        <link>https://en.wikipedia.org/wiki/ACID</link>

        <question id="acid-atomicity">
            <text>Atomicity</text>
            <link>https://en.wikipedia.org/wiki/ACID#Atomicity</link>
            <answer>Either all operations, or no operations, in a transaction succeed.</answer>
        </question>

        <question id="acid-consistency">
            <text>Consistency</text>
            <link>https://en.wikipedia.org/wiki/ACID#Consistency</link>
            <answer>Each transaction changes the state only to another valid state.</answer>
        </question>

        <question id="acid-isolation">
            <text>Isolation</text>
            <link>https://en.wikipedia.org/wiki/ACID#Isolation</link>
            <answer>Concurrent transaction execution results in the same state as if the transactions were executed serially.</answer>
        </question>

        <question id="acid-durability">
            <text>Durability</text>
            <link>https://en.wikipedia.org/wiki/ACID#Durability</link>
            <answer>Once a transaction is committed, the change of state will remain.</answer>
        </question>
    </section>

    <section id="base" answers_as_choices="true">
        <title>BASE</title>
        <link>https://en.wikipedia.org/wiki/Eventual_consistency</link>
        <!-- http://www.dataversity.net/acid-vs-base-the-shifting-ph-of-database-transaction-processing/ -->

        <question id="base-basically-available">
            <text>Basically Available</text>
            <answer>Requests will be served, but could return outdated values.</answer>
        </question>

        <question id="base-soft-state">
            <text>Soft State</text>
            <answer>The system may change state even without external writes. For instance, to achieve eventual consistency.</answer>
        </question>

        <question id="base-eventual-consistency">
            <text>Eventual Consistency</text>
            <answer>Reads will all eventually return the latest value (converge), but consistency is not guaranteed at all times.</answer>
        </question>
    </section>

    <section id="projects" answers_as_choices="true">
        <title>Projects / Products</title>

        <question id="projects-cassandra">
            <text>Cassandra</text>
            <answer>Distributed Storage</answer>
            <link>https://en.wikipedia.org/wiki/Apache_Cassandra</link>
            <note>Shards via consistent hashing. No master (which would be a single point of failure).</note>
        </question>

        <question id="projects-redis">
            <text>Redis</text>
            <answer>Distributed Storage</answer>
            <link>https://en.wikipedia.org/wiki/Redis</link>
            <note>Uses Master/Slave. Doesn't seem to use consistent hashing for sharding, at least by default.</note>
        </question>

        <!-- TODO:
        <question id="projects-mongo-db">
            <text>MongoDB</text>
            <answer>Distributed Storage</answer>
            <link>https://en.wikipedia.org/wiki/MongoDB</link>
        </question>
        -->

        <question id="projects-hadoop">
            <text>Hadoop</text>
            <answer>Distributed Computation: Batch Processing (MapReduce)</answer>
            <link>https://en.wikipedia.org/wiki/Apache_Hadoop</link>
        </question>

        <question id="projects-spark">
            <text>Spark</text>
            <answer>Distributed Computation: Batch Processing</answer>
            <link>https://en.wikipedia.org/wiki/Apache_Spark</link>
            <video_url>https://youtu.be/DpqIc-EggOs?list=PLXCArKglxOG9pWzw23kPY-UcFMCuTLWFs</video_url>
            <note>Unlike Hadoop, Spark tries to keep data in memory, and doesn't restrict processing (transformation) to the MapReduce key/value API.</note>
        </question>

        <question id="projects-storm">
            <text>Storm</text>
            <answer>Distributed Computation: Stream Processing</answer>
            <link>https://en.wikipedia.org/wiki/Storm_(event_processor)</link>
        </question>

        <question id="projects-kafka">
            <text>Kafka</text>
            <answer>Distributed Message Queue</answer>
            <link>https://en.wikipedia.org/wiki/Apache_Kafka</link>
        </question>

        <question id="projects-zookeeper">
            <text>Zookeeper</text>
            <answer>Distributed Configuration and Coordination</answer>
            <link>https://en.wikipedia.org/wiki/Apache_ZooKeeper</link>
        </question>

        <question id="projects-ribbon">
            <text>Ribbon</text>
            <answer>Client-side load balancing library</answer>
            <link>https://github.com/Netflix/Ribbon</link>
            <note>For instance, discovering available service instances via a Eureka service.</note>
        </question>

        <question id="projects-hystrix">
            <text>Hystrix</text>
            <answer>Client-side fault tolerance</answer>
            <link>https://github.com/Netflix/Hystrix</link>
            <note>Including load balancing (via Ribbon) and the circuit breaker, bulkhead, and fallback patterns.</note>
        </question>

        <question id="projects-eureka">
            <text>Eureka</text>
            <answer>Service Discovery</answer>
            <link>https://github.com/Netflix/eureka</link>
            <note>Service instances register with the Eureka service. Clients query Eureka for service instances.</note>
        </question>

        <question id="projects-zuul">
            <text>Zuul</text>
            <answer>Service Gateway (Edge Service)</answer>
            <link>https://github.com/Netflix/zuul</link>
            <note>Routing, logging, authentication, authorization.</note>
        </question>
    </section>

    <!--
    Availability

    Testing:
      Black box, white box.
      Load Testing
      Failure Modes
      Cascading Failures

    Monitoring: Black box, white box.

    Paxos, Kubernetes/Borg

    Subsetting
    Throttling
    Round Robin

    Cascading Failures:
     - Server Overload
     - Resource Exhaustion
     - Service Unavailability.

    Retries
    -->
</quiz>